import json
import os
import re
from dotenv import load_dotenv
from movie_data_manager import MovieDataManager
from mcp_client import MCPClient, MCPMovieToolHandler
from real_mcp_integration import RealMCPMovieSearch
from tavily_search import TavilyMovieSearcher
from llm_client import get_llm_client

# Load environment variables
load_dotenv()

class AgentSupervisor:
    def __init__(self):
        self.movie_manager = MovieDataManager()
        self.conversation_history = [] # ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ëŒ€í™” ê¸°ë¡
        self.last_suggested_movies = [] # ë§ˆì§€ë§‰ìœ¼ë¡œ ì œì•ˆëœ ì˜í™” ëª©ë¡ (top 5)
        self.llm_client = get_llm_client()  # ë©€í‹° LLM í´ë°± í´ë¼ì´ì–¸íŠ¸
        
        # ì‹¤ì œ MCP ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ê°€ì§œ MCP ëŒ€ì²´)
        self.real_mcp = RealMCPMovieSearch(self.movie_manager)
        print(f"ğŸ”— ì‹¤ì œ MCP ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: {self.real_mcp.session_id}")
        
        # ê¸°ì¡´ ê°€ì§œ MCP (í˜¸í™˜ì„± ìœ ì§€ìš©)
        self.mcp_client = MCPClient()
        self.mcp_tool_handler = MCPMovieToolHandler(self.movie_manager)
        self.mcp_session = self.mcp_client.initialize_session()
        
        # Tavily ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.tavily_searcher = TavilyMovieSearcher()
        print("ğŸŒ Tavily ì›¹ ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _add_to_history(self, role, content):
        self.conversation_history.append({"role": role, "content": content})

    def _construct_mcp_request(self, user_input):
        # ì‹¤ì œ MCP í”„ë¡œí† ì½œì„ ì‚¬ìš©í•œ ì»¨í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ìƒì„±
        available_tools = self.mcp_tool_handler.tools
        mcp_context = self.mcp_client.create_context_message(
            conversation_history=self.conversation_history,
            current_input=user_input,
            available_tools=available_tools
        )
        
        # MCP ìƒí˜¸ì‘ìš© ë¡œê¹…
        self.mcp_client.log_mcp_interaction("context_request", mcp_context)
        
        # _send_to_llmì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
        return {
            "current_user_input": user_input,
            "conversation_history": self.conversation_history,
            "mcp_context": mcp_context,
            "available_tools": available_tools
        }

    def _send_to_llm(self, mcp_request):
        """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ LLM ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        user_input = mcp_request["current_user_input"]
        conversation_history = mcp_request["conversation_history"]
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = """
ë‹¹ì‹ ì€ ì˜í™” ì¶”ë¡  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë‹¨í¸ì ì¸ ì •ë³´ë§Œ ì œê³µí•´ë„ ëŒ€í™”ë¥¼ í†µí•´ ì›í•˜ëŠ” ì˜í™”ë¥¼ ì°¾ì•„ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ê·œì¹™:
1. í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”
2. ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì§ˆë¬¸ì„ í•´ì„œ ë” ë§ì€ ì •ë³´ë¥¼ ì–»ìœ¼ì„¸ìš”
3. í˜„ì¬ ì‚¬ìš©ìê°€ ìƒê°í•˜ê³  ìˆì„ ê²ƒ ê°™ì€ ì˜í™” top 5ë¥¼ JSON í˜•íƒœë¡œ ì¶”ì²œí•˜ì„¸ìš”
4. ë°ì´í„°ì…‹ì— ì—†ëŠ” ì˜í™”ë¼ë©´ ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”
5. ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:

{
  "action": "search_movies" ë˜ëŠ” "respond_text",
  "search_params": {
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
    "genre": "ì¥ë¥´",
    "director": "ê°ë…ëª…",
    "actor": "ë°°ìš°ëª…",
    "min_rating": í‰ì ,
    "top_n": 5
  },
  "response_text": "ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ì‘ë‹µ í…ìŠ¤íŠ¸",
  "next_question": "ë‹¤ìŒ ì§ˆë¬¸",
  "reason_no_match": null ë˜ëŠ” "ì´ìœ "
}

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
- search_movies: IMDb ì˜í™” ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ, ì¥ë¥´, ê°ë…, ë°°ìš°, í‰ì ìœ¼ë¡œ ê²€ìƒ‰
"""
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        messages = [{"role": "system", "content": system_prompt}]
        
        for entry in conversation_history:
            if entry["role"] == "user":
                messages.append({"role": "user", "content": entry["content"]})
            elif entry["role"] == "agent":
                messages.append({"role": "assistant", "content": entry["content"]})
        
        # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        messages.append({"role": "user", "content": user_input})
        
        try:
            # ë©€í‹° LLM API í˜¸ì¶œ (í´ë°± ì§€ì›)
            llm_text_response = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            )
            
            # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
            try:
                llm_response = json.loads(llm_text_response)
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
                llm_response = {
                    "action": "respond_text",
                    "response_text": llm_text_response,
                    "next_question": "ë” ìì„¸í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
                    "reason_no_match": None
                }
            
            return llm_response
            
        except Exception as e:
            print(f"LLM API ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ í´ë°± ì‘ë‹µ
            return {
                "action": "respond_text",
                "response_text": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "next_question": "ì–´ë–¤ ì˜í™”ë¥¼ ì°¾ìœ¼ì‹œë‚˜ìš”?",
                "reason_no_match": None
            }


    def _get_gpt_direct_response(self, user_input):
        """GPTê°€ ì§ì ‘ ì œê³µí•˜ëŠ” ì‘ë‹µ"""
        system_prompt = """
ë‹¹ì‹ ì€ í•œêµ­ì¸ ì‚¬ìš©ìë¥¼ ìœ„í•œ ì˜í™” ì¶”ë¡  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìê°€ ë‹¨í¸ì ì¸ ì •ë³´ë§Œ ì œê³µí•´ë„ ëŒ€í™”ë¥¼ í†µí•´ ì˜í™”ë¥¼ ì°¾ë„ë¡ ë„ì™€ì£¼ì„¸ìš”.

ê·œì¹™:
1. í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ë‹µ
2. ì‚¬ìš©ìì˜ ë‹µë³€ ì‹ ë¢°ì„±ì„ í‰ê°€í•˜ê³  í•„ìš”ì‹œ ì¬ì§ˆë¬¸
3. ë°ì´í„°ì…‹ì— ì—†ì„ ê²ƒ ê°™ì€ ì˜í™”ë¼ë©´ ê·¸ ì´ìœ  ì„¤ëª…
4. ì¹œê·¼í•˜ê³  ëŒ€í™”í˜•ìœ¼ë¡œ ì‘ë‹µ
5. ì¶”ê°€ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ í¬í•¨
"""
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨í•´ì„œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
        messages = [{"role": "system", "content": system_prompt}]
        
        # ìµœê·¼ 3í„´ì˜ ëŒ€í™”ë§Œ í¬í•¨ (ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ)
        for entry in self.conversation_history[-6:]:  # user-agent ìŒ 3ê°œ
            if entry["role"] == "user":
                messages.append({"role": "user", "content": entry["content"]})
            elif entry["role"] == "agent":
                messages.append({"role": "assistant", "content": entry["content"]})
        
        # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        messages.append({"role": "user", "content": user_input})
        
        try:
            response = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.7,
                max_tokens=300
            )
            return response
        except Exception as e:
            return f"LLM ì§ì ‘ ì‘ë‹µ ì˜¤ë¥˜: {str(e)}"
    
    def _translate_korean_to_english_keywords(self, user_input):
        """í•œêµ­ì–´ ì…ë ¥ì„ ì˜ì–´ í‚¤ì›Œë“œë¡œ ë³€í™˜"""
        # í•œêµ­ì–´-ì˜ì–´ í‚¤ì›Œë“œ ë§¤í•‘ (ë°œí‘œ ì‹œì—°ìš© í™•ì¥)
        keyword_mapping = {
            "ê°ì˜¥": ["prison", "jail", "shawshank"],
            "íƒˆì¶œ": ["escape", "break", "breakout"],
            "ì•¡ì…˜": ["action"],
            "ë“œë¼ë§ˆ": ["drama"],
            "ì½”ë¯¸ë””": ["comedy"],
            "ë¡œë§¨ìŠ¤": ["romance", "romantic"],
            "ìŠ¤ë¦´ëŸ¬": ["thriller"],
            "ê³µí¬": ["horror"],
            "SF": ["sci-fi", "science fiction"],
            "ìš°ì£¼": ["space", "galaxy"],
            "ì „ìŸ": ["war", "battle"],
            "ë²”ì£„": ["crime", "criminal"],
            "ê°€ì¡±": ["family"],
            "ëª¨í—˜": ["adventure"],
            "ë§ˆí”¼ì•„": ["mafia", "godfather"],
            "ì¢€ë¹„": ["zombie"],
            "ìŠˆí¼íˆì–´ë¡œ": ["superhero", "batman", "superman"],
            "í¬ë¦¬ìŠ¤í† í¼ ë†€ë€": ["Christopher Nolan", "Nolan"],
            "í†° í–‰í¬ìŠ¤": ["Tom Hanks", "Hanks"],
            "ë ˆì˜¤ë‚˜ë¥´ë„ ë””ì¹´í”„ë¦¬ì˜¤": ["Leonardo DiCaprio", "DiCaprio"],
            "ë¸Œë˜ë“œ í”¼íŠ¸": ["Brad Pitt"],
            "ëª¨ê±´ í”„ë¦¬ë¨¼": ["Morgan Freeman"],
            "ì•Œ íŒŒì¹˜ë…¸": ["Al Pacino"],
            "ë¡œë²„íŠ¸ ë“œë‹ˆë¡œ": ["Robert De Niro"],
            "ì¡°ì»¤": ["joker"],
            "ë°°íŠ¸ë§¨": ["batman", "dark knight"],
            "ë°˜ì§€ì˜ ì œì™•": ["lord of the rings", "fellowship"],
            "í•´ë¦¬í¬í„°": ["harry potter", "potter"],
            "íƒ€ì´íƒ€ë‹‰": ["titanic"],
            "ì•„ë°”íƒ€": ["avatar"]
        }
        
        keywords = []
        user_lower = user_input.lower()
        
        for korean, english_list in keyword_mapping.items():
            if korean in user_lower:
                keywords.extend(english_list)
        
        # ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ê°€
        if not keywords:
            if "ì˜í™”" in user_lower:
                keywords = ["movie", "film"]
            else:
                # LLMì„ ì‚¬ìš©í•´ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                try:
                    response = self.llm_client.chat_completion(
                        messages=[
                            {"role": "system", "content": "ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì˜í™” ê²€ìƒ‰ì— ì‚¬ìš©í•  ì˜ì–´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. ìµœëŒ€ 3ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‹µí•˜ì„¸ìš”."},
                            {"role": "user", "content": user_input}
                        ],
                        temperature=0.3,
                        max_tokens=50
                    )
                    llm_keywords = response.strip().split(',')
                    keywords = [k.strip() for k in llm_keywords if k.strip()]
                except:
                    keywords = []
        
        return keywords if keywords else [user_input]  # ìµœì†Œí•œ ì›ë³¸ì´ë¼ë„ ë°˜í™˜

    def _evaluate_mcp_quality(self, user_input, mcp_results):
        """MCP ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ Tavily ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨"""
        if not mcp_results:
            return True  # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›¹ ê²€ìƒ‰
        
        # íŠ¹ì • í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        modern_indicators = [
            "2020", "2021", "2022", "2023", "2024", "2025",
            "ìµœê·¼", "ë„·í”Œë¦­ìŠ¤", "ë””ì¦ˆë‹ˆ", "ë§ˆë¸”", "DC", "ì•„ë§ˆì¡´",
            "ì¢€ë¹„", "ë°”ì´ëŸ¬ìŠ¤", "íŒ¬ë°ë¯¹", "ì½”ë¡œë‚˜", "ë©”íƒ€ë²„ìŠ¤",
            "AI", "ì¸ê³µì§€ëŠ¥", "NFT", "ê°€ìƒí˜„ì‹¤", "VR"
        ]
        
        user_lower = user_input.lower()
        for indicator in modern_indicators:
            if indicator in user_lower:
                print(f"ğŸ” '{indicator}' í‚¤ì›Œë“œ ê°ì§€ - ì›¹ ê²€ìƒ‰ í•„ìš”")
                return True
        
        # MCP ê²°ê³¼ì˜ ì—°ë„ í™•ì¸
        try:
            latest_year = max([int(movie.get('Released_Year', 0)) for movie in mcp_results])
            if "2024" in user_input and latest_year < 2020:
                print(f"ğŸ” ìµœì‹  ì˜í™” ìš”ì²­ì´ì§€ë§Œ MCP ìµœì‹  ê²°ê³¼ê°€ {latest_year}ë…„ - ì›¹ ê²€ìƒ‰ í•„ìš”")
                return True
        except:
            pass
        
        return False  # ì¼ë°˜ì ì¸ ê²½ìš°ëŠ” MCP ê²°ê³¼ ì‚¬ìš©

    def _get_gpt_feedback_on_mcp(self, user_input, mcp_results):
        """GPTê°€ MCP ê²°ê³¼ì— ëŒ€í•´ ì œê³µí•˜ëŠ” í”¼ë“œë°±"""
        if not mcp_results:
            return "MCP ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ì„œ í”¼ë“œë°±ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # MCP ê²°ê³¼ë¥¼ ìš”ì•½
        mcp_summary = []
        for movie in mcp_results[:3]:  # ì²˜ìŒ 3ê°œë§Œ
            mcp_summary.append(f"- {movie['Series_Title']} ({movie['Released_Year']}, í‰ì : {movie['IMDB_Rating']})")
        mcp_text = "\n".join(mcp_summary)
        
        system_prompt = f"""
ë‹¹ì‹ ì€ ì˜í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ "{user_input}"ë¼ê³  ì§ˆë¬¸í–ˆê³ , 
MCP ì‹œìŠ¤í…œì´ ë‹¤ìŒ ì˜í™”ë“¤ì„ ì¶”ì²œí–ˆìŠµë‹ˆë‹¤:

{mcp_text}

ì´ MCP ì¶”ì²œ ê²°ê³¼ì— ëŒ€í•´ í‰ê°€í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”. 
ì¶”ì²œì´ ì ì ˆí•œì§€, ë‹¤ë¥¸ ê´€ì ì—ì„œ ë³¼ ë•ŒëŠ” ì–´ë–¤ì§€, ì¶”ê°€ë¡œ ê³ ë ¤í•  ì ì€ ë¬´ì—‡ì¸ì§€ ë“±ì„ ì•Œë ¤ì£¼ì„¸ìš”.
"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"MCP ì¶”ì²œ ê²°ê³¼ì— ëŒ€í•œ í”¼ë“œë°±ì„ ì£¼ì„¸ìš”."}
        ]
        
        try:
            response = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.7,
                max_tokens=300
            )
            return response
        except Exception as e:
            return f"LLM í”¼ë“œë°± ì˜¤ë¥˜: {str(e)}"

    async def process_request(self, user_input):
        self._add_to_history("user", user_input)
        
        # 1. GPT ì§ì ‘ ì‘ë‹µ
        gpt_direct_response = self._get_gpt_direct_response(user_input)
        
        # 2. MCP ë„êµ¬ ê¸°ë°˜ ê²€ìƒ‰
        mcp_request = self._construct_mcp_request(user_input)
        llm_response = self._send_to_llm(mcp_request)
        
        mcp_movies = []
        mcp_response = ""
        
        # ì‹¤ì œ MCPë¥¼ í†µí•œ ì˜í™” ê²€ìƒ‰ ì‹¤í–‰
        # í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë³€í™˜
        english_keywords = self._translate_korean_to_english_keywords(user_input)
        search_params = {
            "keywords": english_keywords,
            "max_results": 5
        }
        
        # ì‹¤ì œ MCP ë„êµ¬ í˜¸ì¶œ
        mcp_result = await self.real_mcp.call_tool("search_movies", search_params)
        self.real_mcp.log_mcp_interaction("tool_call", {
            "tool": "search_movies", 
            "params": search_params,
            "result_success": "result" in mcp_result
        })
        
        # ì‹¤ì œ MCP ê²°ê³¼ì—ì„œ ì˜í™” ë°ì´í„° ì¶”ì¶œ
        if "result" in mcp_result and "content" in mcp_result["result"]:
            try:
                content_text = mcp_result["result"]["content"][0]["text"]
                mcp_data = json.loads(content_text)
                
                if mcp_data.get("success"):
                    mcp_movies = mcp_data["movies"]
                    self.last_suggested_movies = mcp_movies
                    
                    if mcp_movies:
                        mcp_response = "ğŸ”§ **ì‹¤ì œ MCP ì‹œìŠ¤í…œ ê²€ìƒ‰ ê²°ê³¼:**\n"
                        mcp_response += f"ğŸ“Š ê²€ìƒ‰ëœ ì˜í™”: {mcp_data['count']}ê°œ\n"
                        for i, movie in enumerate(mcp_movies[:3], 1):
                            mcp_response += f"{i}. {movie['Series_Title']} ({movie['Released_Year']}) â­{movie['IMDB_Rating']}\n"
                    else:
                        mcp_response = "ğŸ”§ **ì‹¤ì œ MCP ì‹œìŠ¤í…œ:** ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ì˜í™”ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                else:
                    mcp_response = f"ğŸ”§ **ì‹¤ì œ MCP ì‹œìŠ¤í…œ:** {mcp_data.get('message', 'ê²€ìƒ‰ ì‹¤íŒ¨')}"
                    
            except (json.JSONDecodeError, KeyError, IndexError) as e:
                mcp_response = f"ğŸ”§ **ì‹¤ì œ MCP ì‹œìŠ¤í…œ ì˜¤ë¥˜:** ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ ({str(e)})"
        elif "error" in mcp_result:
            mcp_response = f"ğŸ”§ **ì‹¤ì œ MCP ì‹œìŠ¤í…œ ì˜¤ë¥˜:** {mcp_result['error']['message']}"
        else:
            mcp_response = "ğŸ”§ **ì‹¤ì œ MCP ì‹œìŠ¤í…œ ì˜¤ë¥˜:** ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ í˜•ì‹"
        
        # 3. MCP ê²°ê³¼ í’ˆì§ˆ í™•ì¸ í›„ Tavily ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
        tavily_response = ""
        
        # MCP ê²°ê³¼ í’ˆì§ˆ í‰ê°€ (ê´€ë ¨ì„± í™•ì¸)
        should_use_tavily = self._evaluate_mcp_quality(user_input, mcp_movies)
        
        if len(mcp_movies) < 3 or should_use_tavily:  # MCP ê²°ê³¼ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ì›¹ ê²€ìƒ‰
            print("ğŸŒ MCP ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ Tavily ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
            tavily_result = self.tavily_searcher.search_movie_by_description(
                self.conversation_history, user_input
            )
            
            if tavily_result["success"]:
                tavily_response = "## ğŸŒ Tavily ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n"
                tavily_response += f"**ê²€ìƒ‰ ì¿¼ë¦¬:** {tavily_result['search_query']}\n\n"
                
                for i, movie in enumerate(tavily_result["results"][:3], 1):
                    tavily_response += f"{i}. **{movie['cleaned_title']}** ({movie['source']})\n"
                    tavily_response += f"   ğŸ“ {movie['content']}\n"
                    tavily_response += f"   ğŸ”— [ë” ë³´ê¸°]({movie['url']})\n\n"
            else:
                tavily_response = f"## ğŸŒ Tavily ì›¹ ê²€ìƒ‰ ê²°ê³¼:\nâŒ {tavily_result['error']}\n"
        
        # 4. GPTê°€ MCP ê²°ê³¼ì— ëŒ€í•œ í”¼ë“œë°±
        gpt_feedback = self._get_gpt_feedback_on_mcp(user_input, mcp_movies)
        
        # 5. í†µí•© ì‘ë‹µ ìƒì„± (wish.txt ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ê°œì„ )
        combined_response = f"""ğŸ¬ **ì˜í™” ì¶”ë¡  ê²°ê³¼:**

{gpt_direct_response}

---

{mcp_response}

---

ğŸ’¡ **ì¶”ê°€ ë¶„ì„:**
{gpt_feedback}

ğŸ¤” **ë‹¤ìŒ ì§ˆë¬¸:** í˜¹ì‹œ ê¸°ì–µë‚˜ëŠ” ë‹¤ë¥¸ ë‹¨ì„œê°€ ìˆìœ¼ì‹ ê°€ìš”? (ì¶œì—° ë°°ìš°, ì¤„ê±°ë¦¬, ì¸ìƒ ê¹Šì—ˆë˜ ì¥ë©´ ë“±)"""

        # Tavily ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if tavily_response:
            combined_response += f"""

---

{tavily_response}"""

        self._add_to_history("agent", combined_response)
        return combined_response, mcp_movies

async def main():
    supervisor = AgentSupervisor()
    print("ì˜í™” ì¶”ë¡  ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ì°¾ìœ¼ì‹œëŠ” ì˜í™”ì— ëŒ€í•´ ë‹¨í¸ì ì¸ ì •ë³´ë¥¼ ë§ì”€í•´ì£¼ì„¸ìš”. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)")

    while True:
        user_input = input("ì‚¬ìš©ì: ")
        if user_input.lower() == 'exit':
            print("ì—ì´ì „íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        response, movies = await supervisor.process_request(user_input)
        print(f"ì—ì´ì „íŠ¸: {response}")
        print("-" * 30)

if __name__ == '__main__':
    import asyncio
    asyncio.run(main())
