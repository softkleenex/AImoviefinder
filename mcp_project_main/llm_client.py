"""
ë©€í‹° LLM í´ë¼ì´ì–¸íŠ¸ - OpenAIì™€ Gemini API í´ë°± ì‹œìŠ¤í…œ
"""

import os
import time
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import logging

# API í´ë¼ì´ì–¸íŠ¸ë“¤
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

load_dotenv()

class MultiLLMClient:
    """
    ì—¬ëŸ¬ LLM APIë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•˜ëŠ” í´ë°± ì‹œìŠ¤í…œ
    1. OpenAI GPT-4o-mini (Primary)
    2. OpenAI GPT-4o-mini (Backup Key)
    3. Google Gemini Pro (Final Fallback)
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.current_provider = None
        
        # API í‚¤ ì„¤ì •
        self.openai_keys = [
            os.getenv('OPENAI_API_KEY'),
            os.getenv('OPENAI_API_KEY_BACKUP')
        ]
        self.google_api_key = os.getenv('GOOGLE_API_KEY')
        
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_clients = []
        for key in self.openai_keys:
            if key and OPENAI_AVAILABLE:
                try:
                    client = OpenAI(api_key=key)
                    self.openai_clients.append(client)
                except Exception as e:
                    self.logger.warning(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if self.google_api_key and GEMINI_AVAILABLE:
            try:
                genai.configure(api_key=self.google_api_key)
                self.gemini_client = genai.GenerativeModel('gemini-pro')
                self.logger.info("âœ… Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.gemini_client = None
        else:
            self.gemini_client = None
        
        self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ OpenAI í´ë¼ì´ì–¸íŠ¸: {len(self.openai_clients)}ê°œ")
        self.logger.info(f"Gemini ì‚¬ìš© ê°€ëŠ¥: {'âœ…' if self.gemini_client else 'âŒ'}")
    
    def chat_completion(self, messages: List[Dict[str, str]], 
                       temperature: float = 0.7, max_tokens: int = 1000) -> str:
        """
        ì—¬ëŸ¬ LLMì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•˜ì—¬ ì‘ë‹µ ìƒì„±
        """
        
        # 1. OpenAI í´ë¼ì´ì–¸íŠ¸ë“¤ ìˆœì°¨ ì‹œë„
        for i, client in enumerate(self.openai_clients):
            try:
                self.logger.info(f"ğŸ¤– OpenAI í´ë¼ì´ì–¸íŠ¸ #{i+1} ì‹œë„ ì¤‘...")
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                result = response.choices[0].message.content
                self.current_provider = f"OpenAI-{i+1}"
                self.logger.info(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ #{i+1} ì„±ê³µ")
                return result
                
            except Exception as e:
                self.logger.warning(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ #{i+1} ì‹¤íŒ¨: {str(e)}")
                # Rate limitì´ë‚˜ quota ì´ˆê³¼ ì‹œ ì ì‹œ ëŒ€ê¸°
                if "rate_limit" in str(e).lower() or "quota" in str(e).lower():
                    self.logger.info("Rate limit ê°ì§€, 2ì´ˆ ëŒ€ê¸° í›„ ë‹¤ìŒ ì‹œë„...")
                    time.sleep(2)
                continue
        
        # 2. Gemini í´ë°± ì‹œë„
        if self.gemini_client:
            try:
                self.logger.info("ğŸ”· Gemini í´ë¼ì´ì–¸íŠ¸ ì‹œë„ ì¤‘...")
                
                # OpenAI ë©”ì‹œì§€ í˜•ì‹ì„ Gemini í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                gemini_prompt = self._convert_messages_to_gemini_format(messages)
                
                response = self.gemini_client.generate_content(
                    gemini_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=temperature,
                        max_output_tokens=max_tokens,
                    )
                )
                
                result = response.text
                self.current_provider = "Gemini"
                self.logger.info("âœ… Gemini í´ë¼ì´ì–¸íŠ¸ ì„±ê³µ")
                return result
                
            except Exception as e:
                self.logger.error(f"âŒ Gemini í´ë¼ì´ì–¸íŠ¸ ì‹¤íŒ¨: {str(e)}")
        
        # 3. ëª¨ë“  API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        self.current_provider = "Fallback"
        self.logger.error("ğŸš¨ ëª¨ë“  LLM API ì‹¤íŒ¨, ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    
    def _convert_messages_to_gemini_format(self, messages: List[Dict[str, str]]) -> str:
        """
        OpenAI ë©”ì‹œì§€ í˜•ì‹ì„ Geminiìš© ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
        """
        prompt_parts = []
        
        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")
            
            if role == "system":
                prompt_parts.append(f"System Instructions: {content}")
            elif role == "user":
                prompt_parts.append(f"User: {content}")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}")
        
        return "\n\n".join(prompt_parts)
    
    def get_current_provider(self) -> str:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ LLM ì œê³µì ë°˜í™˜"""
        return self.current_provider or "Unknown"
    
    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            "openai_clients_available": len(self.openai_clients),
            "gemini_available": self.gemini_client is not None,
            "current_provider": self.current_provider,
            "openai_installed": OPENAI_AVAILABLE,
            "gemini_installed": GEMINI_AVAILABLE
        }

# ì „ì—­ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
_global_llm_client = None

def get_llm_client() -> MultiLLMClient:
    """ì „ì—­ LLM í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_llm_client
    if _global_llm_client is None:
        _global_llm_client = MultiLLMClient()
    return _global_llm_client

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_llm_fallback():
    """LLM í´ë°± ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("=== LLM í´ë°± ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    client = get_llm_client()
    print(f"ìƒíƒœ: {client.get_status()}")
    
    # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€
    test_messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì˜í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "ì•¡ì…˜ ì˜í™” ì¶”ì²œí•´ì£¼ì„¸ìš”."}
    ]
    
    try:
        response = client.chat_completion(test_messages)
        print(f"\nâœ… ì‘ë‹µ ì„±ê³µ (ì œê³µì: {client.get_current_provider()})")
        print(f"ì‘ë‹µ: {response[:100]}...")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    test_llm_fallback()